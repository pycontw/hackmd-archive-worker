# 維基教我做的金頭腦：知識圖譜製作初體驗 - 張泰瑋

{%hackmd DR5p-QuLTwylJM8bYjJp1Q %}

* [簡報檔](https://docs.google.com/presentation/d/1ZNtpuWycubxKaz9oskyFemmlqFaNW6cYiIW7Ig0dk_0/edit?usp=sharing)
* [GitHub](https://github.com/udicatnchu/udic-nlp-api)

## 教你做個知識圖

## What can we do with Text-mining
- 用Text Mining來做個知識機器人?
- IBM 2011年, Ontology(知識圖譜)打敗人類冠軍
- Chrome 支援 Ontology 實例![](https://i.imgur.com/YBwXMRH.png)
- ps. 本体、知识库、知识图谱、知识图谱识别之间的关系？(https://www.zhihu.com/question/34835422)

- [Probase](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/05/paper.pdf)
  - 解決 Hearst Pattern 問題的方法: Big Data，iteratively scan web
  - (Domestic animals, cats)的關係很常見的話，就可以確認他們是isA的關係。不斷iterate scan，直到沒有學到新的isA關係為止

- 身為沒有 Big Data 的普通人, 可以利用[維基百科](https://zh.wikipedia.org/wiki/Wikipedia:数据库下载)

## Word2Vec
- 根據字詞的前後文, 把文字轉成向量 (Word2Vec), 透過 Cosine Similarity 可計算字詞間的相關性

- 計算Word2Vec的過程就像磁鐵相吸相斥一樣。一開始所有vector都是隨機方向，透過很多同極性高異極性的磁鐵讓每個vector改變方向。最後方向不再改變時training就結束了
    - 字詞先經過 one-hot encoding 編碼
    - sliding window 掃過文章, 出現在附近的字詞會被訓練為相吸的關係
    - 例如 The quick brown fox jumps這個句子中，brown這個字會被The, quick, fox, jumps這些詞吸過去
    - 掃過大量句子

## Issue with Word2Vec
- 孔子與中國思想家的關聯比魯國還低, 因為中國跟其他詞也常一起出現
- Harmonic Mean: 同時保留 word2vec 與 term frequency 的優點，兩個都要高分Harmonic mean才會高分

## Applications
- TextSum: 抓出一篇長文的重點
    - 可先經過 word2vec 轉化字詞 e.g. 中興大學 -> 中部大學

## Q&A
- 斷詞方法?
    - 結巴
- 與GloVe[（Global Vectors）](https://nlp.stanford.edu/projects/glove/) 的比較 




###### tags: `pycontw2018`
